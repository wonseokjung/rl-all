{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 튜토리얼은 PyTorch를 사용하여 [Gymnasium](https://gymnasium.farama.org)의 CartPole-v1 태스크에서 Deep Q Learning (DQN) 에이전트를 학습시키는 방법을 보여줍니다.\n",
    "\n",
    "태스크\n",
    "\n",
    "에이전트는 카트를 왼쪽 또는 오른쪽으로 움직이는 두 가지 액션 중 하나를 결정해야 합니다. 이를 통해 카트에 부착된 폴이 똑바로 서 있도록 유지해야 합니다. 환경에 대한 더 많은 정보와 다른 도전적인 환경은 [Gymnasium 웹사이트](https://gymnasium.farama.org/environments/classic\\_control/cart\\_pole/)에서 찾을 수 있습니다.\n",
    "\n",
    "![CartPole](https://pytorch.org/tutorials/\\_static/img/cartpole.gif)\n",
    "\n",
    "에이전트가 환경의 현재 상태를 관찰하고 행동을 선택하면, 환경은 새로운 상태로 전이되고 행동의 결과를 나타내는 보상을 반환합니다. 이 태스크에서 보상은 매 타임스텝마다 +1이며, 폴이 너무 많이 기울거나 카트가 중심에서 2.4 유닛 이상 멀어지면 환경이 종료됩니다. 이는 더 잘 수행하는 시나리오가 더 오랜 기간 동안 실행되어 더 큰 반환 값을 누적함을 의미합니다.\n",
    "\n",
    "CartPole 태스크는 에이전트에 대한 입력이 환경 상태(위치, 속도 등)를 나타내는 4개의 실제 값이 되도록 설계되었습니다. 우리는 이러한 4개의 입력을 스케일링 없이 받아들이고, 2개의 출력(각 행동에 대해 하나씩)을 가진 작은 완전 연결 네트워크를 통과시킵니다. 네트워크는 입력 상태가 주어졌을 때 각 행동에 대한 기대 값을 예측하도록 학습됩니다. 그런 다음 기대 값이 가장 높은 행동이 선택됩니다.\n",
    "\n",
    "패키지\n",
    "\n",
    "먼저, 필요한 패키지를 가져옵니다. 우선, [pip]{.title-ref}를 사용하여 설치되는 환경을 위해 [gymnasium](https://gymnasium.farama.org/)이 필요합니다. 이는 원래의 OpenAI Gym 프로젝트를 포크한 것이며 Gym v0.19 이후로 동일한 팀에 의해 유지 관리되고 있습니다. Google Colab에서 실행 중인 경우 다음을 실행하세요:\n",
    "\n",
    "``` {.sourceCode .bash}\n",
    "\n",
    "%%bash\n",
    "\n",
    "pip3 install gymnasium[classic_control]\n",
    "\n",
    "```\n",
    "\n",
    "또한 PyTorch에서 다음을 사용합니다:\n",
    "\n",
    "- 신경망 (`torch.nn`)\n",
    "\n",
    "- 최적화 (`torch.optim`)\n",
    "\n",
    "- 자동 미분 (`torch.autograd`)\n",
    "\n",
    "이 튜토리얼에서는 PyTorch를 사용하여 Gymnasium 환경에서 DQN 에이전트를 학습시키는 과정을 설명합니다. 에이전트는 카트의 좌우 이동을 결정하여 폴을 똑바로 세우는 것이 목표입니다. 에이전트는 환경의 상태를 관찰하고 행동을 선택하면, 환경은 새로운 상태로 전이되고 행동의 결과에 대한 보상을 반환합니다.\n",
    "\n",
    "CartPole 태스크에서는 환경 상태를 나타내는 4개의 실수 값이 에이전트의 입력으로 사용됩니다. 이 입력 값들을 스케일링 없이 작은 완전 연결 신경망에 통과시켜 각 행동에 대한 출력 값을 얻습니다. 신경망은 주어진 상태에서 각 행동의 기대 값을 예측하도록 학습되며, 기대 값이 가장 높은 행동을 선택하게 됩니다.\n",
    "\n",
    "튜토리얼에 필요한 패키지로는 환경을 위한 Gymnasium과 PyTorch의 신경망, 최적화, 자동 미분 기능 등이 사용됩니다. 이를 통해 DQN 에이전트를 구현하고 학습시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "\\=============\n",
    "\n",
    "우리는 DQN 학습을 위해 경험 리플레이 메모리(experience replay memory)를 사용할 것입니다. 이것은 에이전트가 관찰한 전이(transition)를 저장하여 나중에 이 데이터를 재사용할 수 있도록 합니다. 리플레이 메모리에서 무작위로 샘플링함으로써 배치를 구성하는 전이들은 서로 상관관계가 없어집니다. 이렇게 하면 DQN 학습 과정이 크게 안정화되고 개선된다는 것이 입증되었습니다.\n",
    "\n",
    "이를 위해 우리는 두 개의 클래스가 필요합니다:\n",
    "\n",
    "\\- `Transition` - 환경에서 단일 전이를 나타내는 명명된 튜플(named tuple)입니다. 본질적으로 (state, action) 쌍을 (next\\_state, reward) 결과에 매핑합니다. 여기서 state는 나중에 설명할 화면 차이 이미지입니다.\n",
    "\n",
    "\\- `ReplayMemory` - 최근에 관찰된 전이를 보유하는 제한된 크기의 순환 버퍼입니다. 또한 학습을 위해 무작위로 전이 배치를 선택하는 `.sample()` 메서드를 구현합니다.\n",
    "\n",
    "경험 리플레이 메모리는 DQN 에이전트의 학습 과정에서 중요한 역할을 합니다. 에이전트가 환경과 상호 작용하면서 관찰한 전이(state, action, next\\_state, reward)를 저장하고, 이후 학습 시에 이 데이터를 재사용할 수 있도록 합니다.\n",
    "\n",
    "리플레이 메모리에서 무작위로 샘플링하여 전이 배치를 구성하면, 배치 내의 전이들 간의 상관관계를 줄일 수 있습니다. 이는 DQN 학습을 안정화하고 개선하는 데 큰 도움이 됩니다.\n",
    "\n",
    "`Transition` 클래스는 환경에서의 단일 전이를 나타내는 명명된 튜플입니다. (state, action) 쌍을 (next\\_state, reward) 결과에 매핑합니다. 여기서 state는 화면 차이 이미지와 같은 형태로 표현될 수 있습니다.\n",
    "\n",
    "`ReplayMemory` 클래스는 최근에 관찰된 전이를 저장하는 제한된 크기의 순환 버퍼입니다. 새로운 전이가 추가될 때 오래된 전이는 버퍼에서 제거됩니다. 또한, `.sample()` 메서드를 통해 무작위로 전이 배치를 선택하여 학습에 사용할 수 있습니다.\n",
    "\n",
    "이러한 리플레이 메모리를 활용하여 DQN 에이전트를 학습시키면 경험의 효율적인 재사용과 학습의 안정화를 기대할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN 알고리즘\n",
    "\n",
    "\\=============\n",
    "\n",
    "우리가 다루는 환경은 결정론적이므로, 여기서 제시된 모든 방정식은 단순성을 위해 결정론적으로 공식화되었습니다. 강화 학습 문헌에서는 환경에서의 확률적 전이에 대한 기대값도 포함될 것입니다.\n",
    "\n",
    "우리의 목표는 할인된 누적 보상 $R\\_{t\\_0} = \\\\sum\\_{t=t\\_0}^{\\\\infty} \\\\gamma^{t - t\\_0} r\\_t$를 최대화하려는 정책을 학습시키는 것입니다. 여기서 $R\\_{t\\_0}$는 _반환(return)_으로도 알려져 있습니다. 할인율 $\\\\gamma$는 $0$과 $1$ 사이의 상수로, 합이 수렴되도록 보장합니다. 낮은 $\\\\gamma$ 값은 먼 미래의 불확실한 보상을 에이전트가 확신할 수 있는 가까운 미래의 보상보다 덜 중요하게 만듭니다. 또한 에이전트가 먼 미래의 동등한 보상보다 시간적으로 더 가까운 보상을 모으도록 장려합니다.\n",
    "\n",
    "Q-learning의 주요 아이디어는 $Q^\\*: State \\\\times Action \\\\rightarrow \\\\mathbb{R}$라는 함수가 있다면, 즉 주어진 상태에서 행동을 취했을 때 얻게 될 반환값을 알려줄 수 있다면, 보상을 최대화하는 정책을 쉽게 구성할 수 있다는 것입니다:\n",
    "\n",
    "$$\\\\pi^\\*(s) = \\\\arg\\\\!\\\\max\\_a \\\\ Q^\\*(s, a)$$\n",
    "\n",
    "그러나 우리는 세상에 대해 모든 것을 알고 있지 않기 때문에 $Q^\\*$에 접근할 수 없습니다. 하지만 신경망은 범용 함수 근사기이므로, 간단히 하나를 만들어 $Q^\\*$와 유사하도록 학습시킬 수 있습니다.\n",
    "\n",
    "학습 업데이트 규칙으로는 어떤 정책에 대한 모든 $Q$ 함수가 벨만 방정식을 따른다는 사실을 이용할 것입니다:\n",
    "\n",
    "$$Q^{\\\\pi}(s, a) = r + \\\\gamma Q^{\\\\pi}(s', \\\\pi(s'))$$\n",
    "\n",
    "등식의 양쪽 간의 차이를 시간차 오류(temporal difference error) $\\\\delta$라고 합니다:\n",
    "\n",
    "$$\\\\delta = Q(s, a) - (r + \\\\gamma \\\\max\\_a' Q(s', a))$$\n",
    "\n",
    "이 오류를 최소화하기 위해 \\[Huber 손실\\](https://en.wikipedia.org/wiki/Huber\\_loss)을 사용할 것입니다. Huber 손실은 오류가 작을 때는 평균 제곱 오류(MSE)처럼 작동하고, 오류가 클 때는 평균 절대 오류(MAE)처럼 작동합니다. 이렇게 하면 $Q$ 추정값이 매우 노이즈가 있을 때 이상치에 더 강건해집니다. 우리는 리플레이 메모리에서 샘플링된 전이 배치 $B$에 대해 이를 계산합니다:\n",
    "\n",
    "$$\\\\mathcal{L} = \\\\frac{1}{|B|}\\\\sum\\_{(s, a, s', r) \\\\ \\\\in \\\\ B} \\\\mathcal{L}(\\\\delta)$$\n",
    "\n",
    "$$\\\\begin{aligned}\n",
    "\n",
    "\\\\text{where} \\\\quad \\\\mathcal{L}(\\\\delta) = \\\\begin{cases}\n",
    "\n",
    "\\\\frac{1}{2}{\\\\delta^2} & \\\\text{for } |\\\\delta| \\\\le 1, \\\\\\\\\n",
    "\n",
    "|\\\\delta| - \\\\frac{1}{2} & \\\\text{otherwise.}\n",
    "\n",
    "\\\\end{cases}\n",
    "\n",
    "\\\\end{aligned}$$\n",
    "\n",
    "Q-네트워크\n",
    "\n",
    "\\---------\n",
    "\n",
    "우리의 모델은 현재와 이전 화면 패치의 차이를 입력으로 받는 피드 포워드 신경망이 될 것입니다. 출력은 $Q(s, \\\\mathrm{left})$와 $Q(s, \\\\mathrm{right})$의 두 가지이며 ($s$는 네트워크에 대한 입력), 이는 각각 왼쪽과 오른쪽 행동을 취할 때의 _기대 반환값_을 나타냅니다. 결과적으로 신경망은 현재 입력이 주어졌을 때 각 행동을 취함으로써 얻을 수 있는 기대 반환값을 예측하려고 시도합니다.\n",
    "\n",
    "DQN 알고리즘은 Q-learning을 기반으로 하며, 신경망을 사용하여 최적의 Q 함수를 근사합니다. 에이전트는 현재 상태에서 가능한 행동들의 Q 값을 예측하는 신경망을 학습시킵니다. 학습 과정에서는 벨만 방정식을 이용하여 시간차 오류를 계산하고, 이를 최소화하는 방향으로 신경망을 업데이트합니다. 손실 함수로는 Huber 손실을 사용하여 이상치에 강건한 학습을 진행합니다.\n",
    "\n",
    "리플레이 메모리에서 샘플링한 전이 배치를 사용하여 Q 함수를 업데이트하며, 이를 통해 경험을 효율적으로 재사용할 수 있습니다. 최종적으로는 학습된 Q 함수를 이용하여 각 상태에서 Q 값이 최대가 되는 행동을 선택함으로써 최적의 정책을 구할 수 있습니다.\n",
    "\n",
    "이러한 DQN 알고리즘을 통해 에이전트는 주어진 환경에서 최적의 행동 정책을 학습할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "\n",
    "========\n",
    "\n",
    "하이퍼파라미터와 유틸리티\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "이 부분에서는 모델과 옵티마이저를 인스턴스화하고, 몇 가지 유틸리티 함수를 정의합니다:\n",
    "\n",
    "- select_action - 엡실론 탐욕 정책에 따라 행동을 선택합니다. 간단히 말해, 때로는 모델을 사용하여 행동을 선택하고, 때로는 균일하게 샘플링할 것입니다. 무작위 행동을 선택할 확률은 EPS_START에서 시작하여 EPS_END를 향해 지수적으로 감소할 것입니다. EPS_DECAY는 감소 속도를 제어합니다.\n",
    "\n",
    "- plot_durations - 에피소드의 지속 시간을 플롯팅하는 도우미 함수이며, 최근 100개 에피소드의 평균값(공식 평가에서 사용되는 척도)과 함께 표시합니다. 이 플롯은 메인 학습 루프를 포함하는 셀 아래에 위치할 것이며, 매 에피소드 이후 업데이트됩니다.\n",
    "\n",
    "하이퍼파라미터는 모델의 학습을 제어하는 데 사용되는 사용자 정의 매개변수입니다. 여기서는 엡실론 탐욕 정책의 엡실론 값을 조절하는 하이퍼파라미터들이 정의됩니다. 엡실론 값은 초기에는 높은 값(EPS_START)에서 시작하여 점진적으로 낮은 값(EPS_END)으로 감소하며, 감소 속도는 EPS_DECAY에 의해 제어됩니다. 이를 통해 초기에는 탐험을 많이 하다가 점차 학습이 진행됨에 따라 탐욕적인 행동 선택으로 전환하게 됩니다.\n",
    "\n",
    "select_action 함수는 현재 상태에서 엡실론 탐욕 정책에 따라 행동을 선택합니다. 엡실론 확률로 무작위 행동을 선택하고, 그렇지 않은 경우에는 모델을 사용하여 Q 값이 가장 높은 행동을 선택합니다.\n",
    "\n",
    "plot_durations 함수는 에피소드의 지속 시간을 플롯팅하는 도우미 함수입니다. 각 에피소드의 지속 시간과 최근 100개 에피소드의 평균 지속 시간을 함께 표시합니다. 이 플롯을 통해 학습 과정에서 에이전트의 성능 향상을 시각적으로 확인할 수 있습니다.\n",
    "\n",
    "이러한 하이퍼파라미터와 유틸리티 함수들은 DQN 에이전트의 학습 과정을 제어하고 모니터링하는 데 사용됩니다. 이를 통해 학습의 진행 상황을 파악하고, 하이퍼파라미터를 조정하여 최적의 성능을 달성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 02:36:29.961 Python[43671:2369125] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/7b/kknnc53s1w531k3vq6_3br1h0000gn/T/com.apple.python3.savedState\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 루프\n",
    "\n",
    "\\=============\n",
    "\n",
    "마지막으로, 모델을 학습시키는 코드입니다.\n",
    "\n",
    "여기서는 최적화의 단일 단계를 수행하는 `optimize_model` 함수를 찾을 수 있습니다. 먼저 배치를 샘플링하고, 모든 텐서를 하나의 텐서로 연결한 다음, $Q(s\\_t, a\\_t)$와 $V(s\\_{t+1}) = \\\\max\\_a Q(s\\_{t+1}, a)$를 계산하여 손실 함수에 포함시킵니다. 정의에 따라 $s$가 터미널 상태인 경우 $V(s) = 0$으로 설정합니다. 또한 안정성을 높이기 위해 타겟 네트워크를 사용하여 $V(s\\_{t+1})$를 계산합니다. 타겟 네트워크는 이전에 정의된 하이퍼파라미터 `TAU`에 의해 제어되는 \\[소프트 업데이트\\](https://arxiv.org/pdf/1509.02971.pdf)를 사용하여 매 단계마다 업데이트됩니다.\n",
    "\n",
    "학습 루프에서는 `optimize_model` 함수를 사용하여 모델을 최적화합니다. 이 함수는 리플레이 메모리에서 배치를 샘플링하고, 현재 상태에서의 Q 값과 다음 상태에서의 최대 Q 값을 계산하여 손실 함수를 구성합니다. 이때 다음 상태가 터미널 상태인 경우에는 해당 상태의 가치를 0으로 설정합니다.\n",
    "\n",
    "안정적인 학습을 위해 타겟 네트워크를 사용하여 다음 상태의 Q 값을 계산합니다. 타겟 네트워크는 메인 네트워크와 동일한 구조를 가지지만, 가중치 업데이트 속도가 느립니다. 이를 통해 학습 과정에서의 진동을 줄일 수 있습니다. 타겟 네트워크의 가중치는 `TAU` 하이퍼파라미터에 의해 제어되는 소프트 업데이트 방식으로 업데이트됩니다.\n",
    "\n",
    "학습 루프에서는 각 단계마다 `optimize_model` 함수를 호출하여 모델을 업데이트하고, 에피소드가 종료될 때마다 타겟 네트워크를 업데이트합니다. 또한 일정 간격마다 모델의 성능을 평가하고 결과를 출력합니다.\n",
    "\n",
    "이러한 학습 루프를 통해 DQN 에이전트는 환경과의 상호작용을 통해 경험을 쌓고, 리플레이 메모리에 저장된 경험을 활용하여 Q 함수를 학습합니다. 최적화 과정을 반복하면서 에이전트는 점진적으로 성능을 향상시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 루프의 시작에서는 환경을 초기화하고 초기 state 텐서를 얻습니다. 그 다음 행동을 샘플링하고 실행한 후, 다음 상태와 보상(항상 1)을 관찰하고 모델을 한 번 최적화합니다. 에피소드가 끝나면(모델이 실패하면) 루프를 다시 시작합니다.\n",
    "\n",
    "num_episodes는 GPU를 사용할 수 있는 경우 600으로 설정되고, 그렇지 않은 경우 학습 시간이 너무 오래 걸리지 않도록 50 에피소드로 설정됩니다. 그러나 CartPole 환경에서 좋은 성능을 관찰하기에는 50 에피소드로는 충분하지 않을 수 있습니다. 600 에피소드의 학습 과정에서 모델이 지속적으로 500 단계를 달성하는 것을 확인할 수 있어야 합니다.\n",
    "\n",
    "강화 학습 에이전트를 학습시키는 과정은 노이즈가 있을 수 있으므로, 수렴이 관찰되지 않는 경우 학습을 다시 시작하면 더 나은 결과를 얻을 수 있습니다.\n",
    "\n",
    "이러한 학습 루프를 통해 DQN 에이전트는 CartPole 환경에서 최적의 정책을 학습하게 됩니다. 에피소드를 반복하면서 에이전트는 경험을 쌓고, 리플레이 메모리를 활용하여 Q 함수를 업데이트합니다. 학습이 진행됨에 따라 에이전트의 성능은 점진적으로 향상되어, 최종적으로는 CartPole을 오랜 시간 동안 균형 잡힌 상태로 유지할 수 있게 됩니다.\n",
    "\n",
    "충분한 에피소드로 학습을 진행하고, 필요에 따라 학습을 다시 시작하면서 최적의 성능을 달성할 수 있도록 실험해 보는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(state)\n\u001b[0;32m---> 12\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/envs/classic_control/cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    187\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gymnasium/envs/classic_control/cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 10000\n",
    "else:\n",
    "    num_episodes = 5000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/reinforcement_learning_diagram.jpg)\n",
    "\n",
    "\n",
    "먼저 에이전트는 현재 상태에 대해 정책(Policy)에 따라 행동을 선택하거나 무작위로 행동을 선택합니다. 선택된 행동은 gym 환경에 전달되어 다음 상태와 보상을 받아옵니다.\n",
    "\n",
    "이 결과는 리플레이 메모리(Replay Memory)에 저장되며, 매 반복마다 최적화 단계(Optimization Step)가 실행됩니다. 최적화 단계에서는 리플레이 메모리에서 무작위로 배치(Batch)를 선택하여 새로운 정책을 학습합니다. 이때 \"이전\" 타겟 네트워크(Target Network)가 사용되어 기대 Q 값을 계산합니다.\n",
    "\n",
    "학습 과정에서 현재 정책 네트워크(Policy Network)와 타겟 네트워크는 서로 다른 역할을 합니다. 정책 네트워크는 현재 상태에서의 행동을 선택하고, 타겟 네트워크는 다음 상태에서의 Q 값을 예측하는 데 사용됩니다. 이는 학습의 안정성을 높이기 위한 방법으로, 타겟 네트워크의 가중치는 일정 간격마다 정책 네트워크의 가중치로부터 소프트 업데이트됩니다.\n",
    "\n",
    "매 단계마다 정책 네트워크는 리플레이 메모리에서 샘플링된 경험을 기반으로 학습되며, 손실 함수를 최소화하는 방향으로 가중치가 업데이트됩니다. 이를 통해 에이전트는 점진적으로 최적의 정책을 학습하게 됩니다.\n",
    "\n",
    "다이어그램에서는 이러한 과정이 반복되는 것을 볼 수 있습니다. 에이전트는 환경과 상호작용하며 경험을 쌓고, 리플레이 메모리에 저장된 경험을 활용하여 학습을 수행합니다. 학습이 진행됨에 따라 에이전트의 성능은 향상되고, 최적의 정책에 수렴하게 됩니다.\n",
    "\n",
    "전체적인 데이터 흐름은 다음과 같이 요약할 수 있습니다:\n",
    "1. 에이전트가 현재 상태에서 행동을 선택합니다.\n",
    "2. 선택된 행동을 환경에 전달하여 다음 상태와 보상을 받아옵니다.\n",
    "3. 현재 상태, 행동, 보상, 다음 상태를 리플레이 메모리에 저장합니다.\n",
    "4. 리플레이 메모리에서 무작위로 배치를 선택하여 정책 네트워크를 학습합니다.\n",
    "5. 타겟 네트워크를 사용하여 기대 Q 값을 계산하고, 정책 네트워크의 손실 함수를 최소화하는 방향으로 가중치를 업데이트합니다.\n",
    "6. 일정 간격마다 타겟 네트워크의 가중치를 정책 네트워크의 가중치로부터 소프트 업데이트합니다.\n",
    "\n",
    "이러한 과정을 반복하면서 에이전트는 최적의 정책을 학습하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym_super_mario_bros in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes_py in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (8.2.1)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from nes_py) (1.5.21)\n",
      "Requirement already satisfied: gym>=0.17.2 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from nes_py) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from nes_py) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from nes_py) (1.26.4)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from gym>=0.17.2->nes_py) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from gym>=0.17.2->nes_py) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes_py) (3.17.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torch==2.2.2->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torch==2.2.2->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torch==2.2.2->torchvision) (4.9.0)\n",
      "Requirement already satisfied: networkx in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torch==2.2.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: sympy in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torch==2.2.2->torchvision) (1.12)\n",
      "Requirement already satisfied: filelock in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from torch==2.2.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Pillow in /Users/wonseokjung/Library/Python/3.9/lib/python/site-packages (10.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym_super_mario_bros nes_py\n",
    "!pip3 install torchvision\n",
    "!pip3 install --upgrade Pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 훈련 : DQN으로 슈퍼마리오 구현하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
